{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks that 'extract_kpts' class follows\n",
    "\n",
    "\n",
    "## extract_kpts.dataset()\n",
    "    1) Iterate through the yes and no folder.\n",
    "    2) Extract keypoints of of each image and store them in a numpy array\n",
    "       of each class folder respectively.\n",
    "## extract_kpts.inference_img()\n",
    "    For inferencing on images.\n",
    "## extract_kpts.inference_vid()\n",
    "    For inferencing on videos.\n",
    "**Note :** Below functions take large amount of time in creating the dataset (~ 25mins) and inferencing videos (~ 5mins). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "class extract_kpts:\n",
    "\n",
    "\n",
    "    def preprocess(img):\n",
    "        frame = cv2.imread(img)\n",
    "        frameCopy = np.copy(frame)\n",
    "        frameWidth = frame.shape[1]\n",
    "        frameHeight = frame.shape[0]\n",
    "        aspect_ratio = frameWidth/frameHeight\n",
    "\n",
    "        inHeight = 368\n",
    "        inWidth = int(((aspect_ratio*inHeight)*8)//8)\n",
    "\n",
    "        return inHeight, inWidth, frameWidth, frameHeight, frame\n",
    "    def preprocess_vid(img):\n",
    "        frameWidth = img.shape[1]\n",
    "        frameHeight = img.shape[0]\n",
    "        aspect_ratio = frameWidth/frameHeight\n",
    "\n",
    "        inHeight = 368\n",
    "        inWidth = int(((aspect_ratio*inHeight)*8)//8)\n",
    "\n",
    "        return inHeight, inWidth, frameWidth, frameHeight\n",
    "\n",
    "\n",
    "    def dataset(protoFile,weightsFile,path,threshold=0):\n",
    "        points = []\n",
    "        labels = []\n",
    "        nPoints = 22\n",
    "        imagePaths = list(paths.list_images(path))\n",
    "        net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "        # loop over the image paths\n",
    "        for imagePath in imagePaths:\n",
    "            # extract the class label from the filename\n",
    "            label = imagePath.split(os.path.sep)[-2]\n",
    "            # load the input image (224x224) and preprocess it\n",
    "            img_h, img_w, frameWidth, frameHeight, img = extract_kpts.preprocess(imagePath)\n",
    "            inpBlob = cv2.dnn.blobFromImage(img, 1.0 / 255, (img_w, img_h), (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "            net.setInput(inpBlob)\n",
    "\n",
    "            output = net.forward()  \n",
    "\n",
    "            for i in range(nPoints):\n",
    "                # confidence map of corresponding body's part.\n",
    "                probMap = output[0, i, :, :]\n",
    "                probMap = cv2.resize(probMap, (frameWidth, frameHeight))\n",
    "\n",
    "                # Find global maxima of the probMap. to determine location of Keypoint\n",
    "                minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "                if prob > threshold :\n",
    "                    # Add the point to the list if the probability is greater than the threshold\n",
    "                    points.append((int(point[0]), int(point[1])))\n",
    "                    if label == 'yes':\n",
    "                        labels.append(1)\n",
    "                    else:\n",
    "                        labels.append(0)\n",
    "                else :\n",
    "                    points.append(None) \n",
    "\n",
    "        points = np.array(points)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        return points, labels\n",
    "    \n",
    "    def save_file():\n",
    "        # 35,376\n",
    "        # [0 : 17687] - 0\n",
    "        # [17688 : ]  - 1\n",
    "        # 35376/44 = 804\n",
    "        # 804 * 2 = 1608\n",
    "\n",
    "        protoFile = \"hand/pose_deploy.prototxt\"\n",
    "        WeightsFile = \"hand/pose_iter_102000.caffemodel\"\n",
    "\n",
    "\n",
    "        if os.path.isfile('final_dataset_sample.csv') is False: \n",
    "\n",
    "            points, labels = extract_kpts.dataset(protoFile,WeightsFile,path)\n",
    "            data = pd.DataFrame()\n",
    "            classes = pd.DataFrame(labels, columns=['class'])\n",
    "\n",
    "            # Change the column names accordingly in future dataset\n",
    "            points = pd.DataFrame(points, columns=['X','Y'])\n",
    "\n",
    "            data = pd.concat([points, classes], axis=1)\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Making 44 columns(22*X,22*Y) using every 22 rows(samples)\n",
    "            a = df[['X','Y']].to_numpy().reshape(-1, 44)\n",
    "\n",
    "            df1 = pd.DataFrame(a)\n",
    "            df1['class'] = df['class'].to_numpy().reshape(a.shape[0], -1)[:, 0]\n",
    "\n",
    "            df1.to_csv('final_dataset_sample.csv',index=False)\n",
    "    \n",
    "    def inference_img(protoFile,weightsFile,imagePath):\n",
    "        nPoints = 22\n",
    "        threshold=0\n",
    "        points=[]\n",
    "        net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "        img_h, img_w, frameWidth, frameHeight, img = extract_kpts.preprocess(imagePath)\n",
    "        inpBlob = cv2.dnn.blobFromImage(img, 1.0 / 255, (img_w, img_h), (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "        net.setInput(inpBlob)\n",
    "\n",
    "        output = net.forward()  \n",
    "        for i in range(nPoints):\n",
    "            # confidence map of corresponding body's part.\n",
    "            probMap = output[0, i, :, :]\n",
    "            probMap = cv2.resize(probMap, (frameWidth, frameHeight))\n",
    "\n",
    "            # Find global maxima of the probMap. to determine location of Keypoint\n",
    "            minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "            if prob > threshold :\n",
    "                # Add the point to the list if the probability is greater than the threshold\n",
    "                points.append((int(point[0]), int(point[1])))\n",
    "        points = np.array(points)\n",
    "\n",
    "\n",
    "        return points    \n",
    "    \n",
    "    def inference_vid(args,path,protoFile,weightsFile,vidPath,model):\n",
    "        \n",
    "        cap = cv2.VideoCapture(path)\n",
    "        hasFrame, frame = cap.read()\n",
    "        net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "        nPoints = 22\n",
    "        threshold=0\n",
    "        points=[]      \n",
    "        dir = os.path.join(args['output_dir']+'Output_vid.avi')\n",
    "        vid_writer = cv2.VideoWriter(dir,cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame.shape[1],frame.shape[0]))\n",
    "        while hasFrame:\n",
    "                       \n",
    "            hasFrame, frame = cap.read()\n",
    "            img_h, img_w, frameWidth, frameHeight = extract_kpts.preprocess_vid(frame) \n",
    "            if not hasFrame:\n",
    "                cv2.waitKey()\n",
    "                break\n",
    "            \n",
    "            inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (img_w, img_h), (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "            net.setInput(inpBlob)\n",
    "\n",
    "            output = net.forward()  \n",
    "            for i in range(nPoints):\n",
    "                # confidence map of corresponding body's part.\n",
    "                probMap = output[0, i, :, :]\n",
    "                probMap = cv2.resize(probMap, (frameWidth, frameHeight))\n",
    "\n",
    "                # Find global maxima of the probMap. to determine location of Keypoint\n",
    "                minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "                if prob > threshold :\n",
    "                    # Add the point to the list if the probability is greater than the threshold\n",
    "                    points.append((int(point[0]), int(point[1])))\n",
    "                if ((i+1)%22==0):\n",
    "                    points = np.array(points)\n",
    "                    \n",
    "                    points = pd.DataFrame(points, columns=['X','Y'])\n",
    "                    df = pd.DataFrame(points)\n",
    "                    a = df[['X','Y']].to_numpy().reshape(-1, 44)\n",
    "\n",
    "                    df1 = pd.DataFrame(a)\n",
    "                    points = []\n",
    "\n",
    "                    frame=cv2.putText(frame,\"Output : {}\".format(np.argmax(model.predict(df1),axis=1)[0]),(0,30),\n",
    "                    cv2.FONT_ITALIC,1.3,(255,0,0),3)\n",
    "                    \n",
    "            cv2.imshow('frame', frame)    \n",
    "            \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord(\"q\"):\n",
    "                break\n",
    "            vid_writer.write(frame)\n",
    "        cap.release()        \n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imutils import paths\n",
    "# from tensorflow.keras.preprocessing.image import load_img\n",
    "# from tensorflow.keras.preprocessing.image import img_to_array\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "\n",
    "# def preprocess(img):\n",
    "#     frame = cv2.imread(img)\n",
    "#     frameCopy = np.copy(frame)\n",
    "#     frameWidth = frame.shape[1]\n",
    "#     frameHeight = frame.shape[0]\n",
    "#     aspect_ratio = frameWidth/frameHeight\n",
    "\n",
    "#     inHeight = 368\n",
    "#     inWidth = int(((aspect_ratio*inHeight)*8)//8)\n",
    "    \n",
    "#     return inHeight, inWidth, frameWidth, frameHeight, frame\n",
    "\n",
    "\n",
    "# def dataset(path):\n",
    "#     protoFile = \"hand/pose_deploy.prototxt\"\n",
    "#     weightsFile = \"hand/pose_iter_102000.caffemodel\"\n",
    "#     imagePaths = list(paths.list_images(path))\n",
    "#     threshold = 0\n",
    "#     points = []\n",
    "#     labels = []\n",
    "#     nPoints = 22\n",
    "#     net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "#     # loop over the image paths\n",
    "#     for imagePath in imagePaths:\n",
    "#         # extract the class label from the filename\n",
    "#         label = imagePath.split(os.path.sep)[-2]\n",
    "#         # load the input image (224x224) and preprocess it\n",
    "#         img_h, img_w, frameWidth, frameHeight, img = preprocess(imagePath)\n",
    "#         inpBlob = cv2.dnn.blobFromImage(img, 1.0 / 255, (img_w, img_h), (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "#         net.setInput(inpBlob)\n",
    "\n",
    "#         output = net.forward()  \n",
    "\n",
    "#         for i in range(nPoints):\n",
    "#             # confidence map of corresponding body's part.\n",
    "#             probMap = output[0, i, :, :]\n",
    "#             probMap = cv2.resize(probMap, (frameWidth, frameHeight))\n",
    "\n",
    "#             # Find global maxima of the probMap. to determine location of Keypoint\n",
    "#             minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "#             if prob > threshold :\n",
    "#                 # Add the point to the list if the probability is greater than the threshold\n",
    "#                 points.append((int(point[0]), int(point[1])))\n",
    "#                 #cv2.putText(frameCopy, \"{}\".format(i), (int(point[0]), int(point[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n",
    "                \n",
    "#                 if label == 'yes':\n",
    "#                     labels.append(1)\n",
    "#                 else:\n",
    "#                     labels.append(0)\n",
    "#             else :\n",
    "#                 points.append(None) \n",
    "        \n",
    "\n",
    "#     points = np.array(points)\n",
    "#     labels = np.array(labels)\n",
    "    \n",
    "#     return points, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_file():\n",
    "#         protoFile = \"hand/pose_deploy.prototxt\"\n",
    "#         WeightsFile = \"hand/pose_iter_102000.caffemodel\"\n",
    "#         #     imagePaths = list(paths.list_images(path))\n",
    "#         threshold = 0\n",
    "#         if os.path.isfile('KPts_dataset_2.csv') is False:\n",
    "#             path = os.getcwd()+'\\\\dataset'\n",
    "\n",
    "#             dataset = extract_kpts.dataset(protoFile,WeightsFile,path)\n",
    "\n",
    "#             # Extracting points and labels np arrays\n",
    "#             points = dataset[0]\n",
    "#             labels = dataset[1]\n",
    "\n",
    "#             classes = labels.copy()\n",
    "#             classes = pd.DataFrame(classes, columns=['class'])\n",
    "\n",
    "#             # Change the column names accordingly in future dataset\n",
    "#             data = pd.DataFrame(points, columns=['X','Y'])\n",
    "#             # data.to_csv('data_XY.csv', index=False)\n",
    "\n",
    "#             # Concatenating both classes and Keypoints and saving the dataset\n",
    "#             dataset = pd.concat([data,classes],axis=1)\n",
    "#             dataset.to_csv('KPts_dataset_2.csv',index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('KPts_dataset_2.csv')\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# a = df[['X','Y']].to_numpy().reshape(-1, 44)\n",
    "       \n",
    "# df1 = pd.DataFrame(a)\n",
    "# df1['class'] = df['class'].to_numpy().reshape(a.shape[0], -1)[:, 0]\n",
    "\n",
    "# data_2 = pd.DataFrame(df1)\n",
    "# dataset = pd.concat([data_1, data_2], axis=0)\n",
    "# dataset.to_csv('final_dataset_sample.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
